{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_step1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPwNLT6gZjOe",
        "colab_type": "text"
      },
      "source": [
        "# Table of contents\n",
        "\n",
        "\n",
        "1.   import libraries\n",
        "2.   download data & extract file\n",
        "3.   Read File Name & Set label & Clean Data\n",
        "4.   Read Dataset\n",
        "5.   Image Augmenation\n",
        "6.   Randomize our data\n",
        "7.   Using KFOLD to split our data and labels\n",
        "8.   Create Train and Test Dataloaders\n",
        "\n",
        "\n",
        "\n",
        "9. Define Model\n",
        "\n",
        "> * CNN3DModel\n",
        "\n",
        "> * ResNet 3D\n",
        "\n",
        "> * DensNet 3D\n",
        "\n",
        "> * Encoder Decoder (ConvLSTM)\n",
        "\n",
        "\n",
        "10. Generating Model\n",
        "\n",
        "11. Set train model & other parameters\n",
        "\n",
        "12. Train model\n",
        "\n",
        "13. visualization loss & accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaN0iYCEWrb1",
        "colab_type": "text"
      },
      "source": [
        "# ***Import library***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGqPerb65mYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pnd\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import math\n",
        "import os\n",
        "import zipfile\n",
        "import six\n",
        "import warnings\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import cv2\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "from imageio import imwrite as iw\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as Data\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8O9OdpXW3Ru",
        "colab_type": "text"
      },
      "source": [
        "# **Download Data & Extract File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dfdZIK8mNxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://ucd5e847d769b25a95722e606559.dl.dropboxusercontent.com/zip_download_get/AhKqMxLj7n8mrkmKUTgJbI7LLn_EZnv5Pe6Lq9FpDCmdp3m_l3gDf8YM11KUvbvBody8_SY8nDrz4Wk194deR9k8l2GOy0YM4hl_EPZVup9IJQ?_download_id=49996537314264544023290189576709031930841558608453748767355469244067&_notify_domain=www.dropbox.com&dl=1\"\n",
        "target_path = '/content/brain.zip'\n",
        "import requests, zipfile, io\n",
        "response = requests.get(url, stream=True)\n",
        "handle = open(target_path, \"wb\")\n",
        "for chunk in response.iter_content(chunk_size=100):\n",
        "    if chunk:  # filter out keep-alive new chunks\n",
        "        handle.write(chunk)\n",
        "handle.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPwUfMGv92tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/brain.zip', 'r')\n",
        "zip_ref.extractall('/content/home/train')\n",
        "zip_ref.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84Z-akE-Oe55",
        "colab_type": "text"
      },
      "source": [
        "# Read File Name & Set label & Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnOIm4-t3_WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "road = '/content/home/train/brain4cars_data/road_camera/' \n",
        "face = '/content/home/train/brain4cars_data/face_camera/'\n",
        "classes = os.listdir(face) # face and road have the same classes\n",
        "\n",
        "face_filename=[]\n",
        "road_filename=[]\n",
        "labels=[]\n",
        "\n",
        "for i in range(len(classes)):\n",
        "  path_face = face + classes[i]\n",
        "  path_road = road + classes[i]\n",
        "  face_check = os.listdir(path_face)\n",
        "  road_check = os.listdir(path_road)\n",
        "\n",
        "  for j in range(len(face_check)):\n",
        "    if face_check[j]+'.avi' in road_check:\n",
        "      video_face_path = path_face+'/'+face_check[j]+'/video_'+face_check[j]+'.avi'\n",
        "      video_road_path = path_road+'/'+face_check[j]+'.avi'\n",
        "      \n",
        "      try:\n",
        "        clip_face = VideoFileClip(video_face_path)\n",
        "        clip_road = VideoFileClip(video_road_path)\n",
        "        a = clip_face.duration\n",
        "        b = clip_road.duration\n",
        "      except:\n",
        "        a = 1 \n",
        "        b = 2\n",
        "      if a==b and a>5:\n",
        "        face_filename.append(video_face_path)\n",
        "        road_filename.append(video_road_path)\n",
        "        labels.append(classes[i])\n",
        "        gc.collect()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcZ6vtPbXdiw",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DWRG15dbtQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BrainforCarsDataset(Dataset):\n",
        "    def __init__(self, face_filename, road_filename, labels, input_size, sample_rate, num_frames, transform=None):\n",
        "        self.face_filename = face_filename\n",
        "        self.road_filename = road_filename\n",
        "        self.transform = transform\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.sample_rate = sample_rate\n",
        "        self.input_size = input_size\n",
        "        self.num_imgs = len(self.face_filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_imgs\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        count =0\n",
        "        data_face=[]\n",
        "        cap = cv2.VideoCapture(self.face_filename[idx])\n",
        "        if not cap.isOpened():\n",
        "            print(\"Unable to connect to camera.\")\n",
        "        while cap.isOpened():\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "            \n",
        "            if ret == True and count%self.sample_rate==0 :\n",
        "              frame = cv2.resize(frame, (self.input_size, self.input_size), interpolation = cv2.INTER_AREA)\n",
        "              data_face.append(frame)\n",
        "\n",
        "            if ret == False or len(data_face)==self.num_frames:\n",
        "              break\n",
        "            count=count+ 1\n",
        "\n",
        "        \n",
        "        count =0\n",
        "        data_road=[]\n",
        "        cap = cv2.VideoCapture(self.road_filename[idx])\n",
        "        if not cap.isOpened():\n",
        "            print(\"Unable to connect to camera.\")\n",
        "        while cap.isOpened():\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "            if ret == True and count%self.sample_rate==0 :\n",
        "              frame = cv2.resize(frame, (self.input_size, self.input_size), interpolation = cv2.INTER_AREA)\n",
        "              data_road.append(frame)\n",
        "\n",
        "            if ret == False or len(data_road)== self.num_frames :\n",
        "              break\n",
        "            count=count+ 1\n",
        "\n",
        "        lm = self.labels[idx]\n",
        "        data_face = np.array(data_face)\n",
        "        data_road = np.array(data_road)\n",
        "        sample = {'image_face': data_face,'image_road': data_road, 'label': lm}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKD3W9MHZrj_",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations (all of these Augmentatios dont change the labels of the data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt1rrlfV9N5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImgAugTransform(object):\n",
        "  def __init__(self):\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
        "    self.aug = iaa.Sequential(\n",
        "        [\n",
        "            # apply the following augmenters to most images\n",
        "            iaa.LinearContrast((2.0, 2.5)), \n",
        "            iaa.Invert(1, per_channel=True), \n",
        "            sometimes(iaa.Affine(\n",
        "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n",
        "                rotate=(-10, 10), # rotate by -45 to +45 degrees\n",
        "                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
        "                \n",
        "                mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "            )),\n",
        "            # execute 0 to 5 of the following (less important) augmenters per image\n",
        "            # don't execute all of them, as that would often be way too strong\n",
        "            iaa.SomeOf((0, 5),\n",
        "                [\n",
        "                    \n",
        "                    iaa.OneOf([\n",
        "                        iaa.GaussianBlur((0, 0.5)), # blur images with a sigma between 0 and 3.0\n",
        "                        iaa.AverageBlur(k=(1, 3)), # blur image using local means with kernel sizes between 2 and 7\n",
        "                        iaa.MedianBlur(k=(1, 3)), # blur image using local medians with kernel sizes between 2 and 7\n",
        "                    ]),\n",
        "                    iaa.Sharpen(alpha=(.9, 1.0), lightness=(0.5, 1.6)), # sharpen images\n",
        "                    \n",
        "                    # search either for all edges or for directed edges,\n",
        "                    # blend the result with the original image using a blobby mask\n",
        "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "                        iaa.EdgeDetect(alpha=(0.0, 0.2)),\n",
        "                        \n",
        "                    ])),\n",
        "                    \n",
        "                    iaa.OneOf([\n",
        "                        iaa.Dropout((0.01, 0.03), per_channel=0.5), # randomly remove up to 10% of the pixels                        \n",
        "                    ]),\n",
        "                    iaa.Invert(0.01, per_channel=True), # invert color channels\n",
        "                    iaa.Add((-2, 2), per_channel=0.5), # change brightness of images (by -2 to 2 of original value)\n",
        "                    iaa.AddToHueAndSaturation((-1, 1)), # change hue and saturation - add blue light\n",
        "                    # either change the brightness of the whole image (sometimes\n",
        "                    # per channel) or change the brightness of subareas\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "                        iaa.FrequencyNoiseAlpha( exponent=(-1, 0),first=iaa.Multiply((0.9, 1.1), per_channel=True),  # add dark light\n",
        "                        second=iaa.ContrastNormalization((0.5, 1.5))\n",
        "                        )\n",
        "                    ]),\n",
        "                    sometimes(iaa.ElasticTransformation(alpha=(0.3, 0.5), sigma=0.2)), # move pixels locally around (with random strengths)\n",
        "                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.02))), # sometimes move parts of the image around\n",
        "                    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.05))) # change perspective\n",
        "                ],\n",
        "                random_order=True\n",
        "            )\n",
        "        ],\n",
        "        random_order=True\n",
        "    )\n",
        "      \n",
        "  def __call__(self, sample):\n",
        "    img_face = sample['image_face']\n",
        "    img_face = img.astype(np.uint8)  #imgaug works with np.unit8\n",
        "    img_face = torch.from_numpy(self.aug.augment_image(img_face).copy())\n",
        "\n",
        "    img_road = sample['image_road']\n",
        "    img_road = img_road.astype(np.uint8)\n",
        "    img_road = torch.from_numpy(self.aug.augment_image(img_road).copy())\n",
        "\n",
        "    sample_1 = {'image_face': img_face, 'image_road': img_road, 'label':sample['label']}\n",
        "    return sample1 \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hmo-ORRf3p",
        "colab_type": "text"
      },
      "source": [
        "# Randomize our data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8oExICiOzFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(1254)\n",
        "\n",
        "combined = list(zip(face_filename, road_filename, labels))\n",
        "random.shuffle(combined)\n",
        "\n",
        "face_filename, road_filename,labels = zip(*combined)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VuUu2LqRuxE",
        "colab_type": "text"
      },
      "source": [
        "# Using KFOLD to split our data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVO_yXHOS-Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold = 0\n",
        "kf = KFold(n_splits=5, shuffle=False, random_state=42)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n",
        "face_filename = np.array(face_filename)\n",
        "road_filename = np.array(road_filename)\n",
        "labels = np.array(labels)\n",
        "for i ,(train_indices, test_indices) in enumerate(skf.split(face_filename, labels)):\n",
        "  if i ==n_fold:\n",
        "    face_filename_train = face_filename[train_indices]\n",
        "    face_filename_test = face_filename[test_indices]\n",
        "    road_filename_train = road_filename[train_indices]\n",
        "    road_filename_test = road_filename[test_indices]\n",
        "    labels_train = labels[train_indices]\n",
        "    labels_test = labels[test_indices]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEHDy_O-SqWw",
        "colab_type": "text"
      },
      "source": [
        "# Train and Test Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewr5atlRVPgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_loader(input_size, sample_rate, num_frames):\n",
        "    ImgAug = ImgAugTransform()\n",
        "\n",
        "\n",
        "    \n",
        "    composed = transforms.Compose([ImgAug])\n",
        "    train_data = BrainforCarsDataset(face_filename_train,road_filename_train, labels_train, input_size, sample_rate, num_frames, transform=composed)\n",
        "    train_loader = Data.DataLoader(train_data, batch_size=1, shuffle=False, num_workers=0)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def get_test_loader(input_size, sample_rate, num_frames):\n",
        "    test_data = BrainforCarsDataset(face_filename_test, road_filename_test, labels_test, input_size, sample_rate, num_frames, transform=None)\n",
        "    test_loader = Data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0)\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "train_loader = get_train_loader(224, 5, 30)\n",
        "test_loader = get_test_loader(224, 5, 30)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXDT27hYbUXJ",
        "colab_type": "text"
      },
      "source": [
        "# Define Model\n",
        "\n",
        "> CNN3DModel\n",
        "\n",
        "> ResNet 3D\n",
        "\n",
        "> DensNet 3D\n",
        "\n",
        "> Encoder Decoder (ConvLSTM)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7SmlnZkKx3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN3DModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN3DModel, self).__init__()\n",
        "        \n",
        "        self.conv_layer1 = nn.Sequential(\n",
        "        nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=0),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool3d((2, 2, 2)),\n",
        "        )\n",
        "        self.conv_layer2 = nn.Sequential(\n",
        "        nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=0),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool3d((2, 2, 2)),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(746496, 128)\n",
        "        self.fc2 = nn.Linear(128, 5)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        #self.batch =nn.BatchNorm1d(128)\n",
        "        self.drop=nn.Dropout(p=0.15)        \n",
        "    \n",
        "    def forward(self, face, road):\n",
        "        # Set 1\n",
        "        out = self.conv_layer1(face)\n",
        "        out = self.conv_layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out1 = self.conv_layer1(road)\n",
        "        out1 = self.conv_layer2(out1)\n",
        "        out1 = out.view(out1.size(0), -1)\n",
        "\n",
        "        out = torch.cat((out,out1),dim=1)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        #print(out.size())\n",
        "        #out = self.batch(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fc2(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igJjsfR_bSGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "__all__ = ['ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnet200']\n",
        "\n",
        "\n",
        "def conv3x3x3(in_planes, out_planes, stride=1):\n",
        "    # 3x3x3 convolution with padding\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=3,\n",
        "                     stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "def downsample_basic_block(x, planes, stride):\n",
        "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "    zero_pads = torch.Tensor(out.size(0), planes - out.size(1),\n",
        "                             out.size(2), out.size(3),\n",
        "                             out.size(4)).zero_()\n",
        "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
        "        zero_pads = zero_pads.cuda()\n",
        "\n",
        "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type='B', num_classes=400, last_fc=True):\n",
        "        self.last_fc = last_fc\n",
        "\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2),\n",
        "                               padding=(3, 3, 3), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n",
        "        last_duration = math.ceil(sample_duration / 16)\n",
        "        last_size = math.ceil(sample_size / 32)\n",
        "        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(downsample_basic_block,\n",
        "                                     planes=planes * block.expansion,\n",
        "                                     stride=stride)\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv3d(self.inplanes, planes * block.expansion,\n",
        "                              kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm3d(planes * block.expansion)\n",
        "                )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if self.last_fc:\n",
        "            x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_fine_tuning_parameters(model, ft_begin_index):\n",
        "    if ft_begin_index == 0:\n",
        "        return model.parameters()\n",
        "\n",
        "    ft_module_names = []\n",
        "    for i in range(ft_begin_index, 5):\n",
        "        ft_module_names.append('layer{}'.format(ft_begin_index))\n",
        "    ft_module_names.append('fc')\n",
        "\n",
        "    parameters = []\n",
        "    for k, v in model.named_parameters():\n",
        "        for ft_module in ft_module_names:\n",
        "            if ft_module in k:\n",
        "                parameters.append({'params': v})\n",
        "                break\n",
        "        else:\n",
        "            parameters.append({'params': v, 'lr': 0.0})\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def resnet10(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet34(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet101(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet152(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet200(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzn4RGsJbRyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet264']\n",
        "\n",
        "\n",
        "def get_fine_tuning_parameters(model, ft_begin_index):\n",
        "    if ft_begin_index == 0:\n",
        "        return model.parameters()\n",
        "\n",
        "    ft_module_names = []\n",
        "    for i in range(ft_begin_index, 5):\n",
        "        ft_module_names.append('denseblock{}'.format(ft_begin_index))\n",
        "        ft_module_names.append('transition{}'.format(ft_begin_index))\n",
        "    ft_module_names.append('norm5')\n",
        "    ft_module_names.append('classifier')\n",
        "\n",
        "    parameters = []\n",
        "    for k, v in model.named_parameters():\n",
        "        for ft_module in ft_module_names:\n",
        "            if ft_module in k:\n",
        "                parameters.append({'params': v})\n",
        "                break\n",
        "        else:\n",
        "            parameters.append({'params': v, 'lr': 0.0})\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Sequential):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm3d(num_input_features))\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv1', nn.Conv3d(num_input_features, bn_size * growth_rate,\n",
        "                                            kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv2', nn.Conv3d(bn_size * growth_rate, growth_rate,\n",
        "                                            kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_features = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return torch.cat([x, new_features], 1)\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv3d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    \"\"\"Densenet-BC model class\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_size, sample_duration, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, last_fc=True):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        self.last_fc = last_fc\n",
        "\n",
        "        self.sample_size = sample_size\n",
        "        self.sample_duration = sample_duration\n",
        "\n",
        "        # First convolution\n",
        "        # self.features = nn.Sequential(OrderedDict([\n",
        "        #     ('conv0', nn.Conv3d(3, num_init_features, kernel_size=7,\n",
        "        #                         stride=(1, 2, 2), padding=(3, 3, 3), bias=False)),\n",
        "        #     ('norm0', nn.BatchNorm3d(num_init_features)),\n",
        "        #     ('relu0', nn.ReLU(inplace=True)),\n",
        "        #     ('pool0', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)),\n",
        "        # ]))\n",
        "\n",
        "\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(3, num_init_features, kernel_size=7,\n",
        "                                stride=(1, 2, 2), padding=(3, 3, 3), bias=False),\n",
        "            nn.BatchNorm3d(num_init_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        last_duration = math.ceil(self.sample_duration / 16)\n",
        "        last_size = math.floor(self.sample_size / 32)\n",
        "        out = F.avg_pool3d(out, kernel_size=(last_duration, last_size, last_size)).view(features.size(0), -1)\n",
        "        if self.last_fc:\n",
        "            out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "def densenet121(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                     **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet169(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n",
        "                     **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet201(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n",
        "                     **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet264(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 64, 48),\n",
        "                     **kwargs)\n",
        "    return model\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH1cX9LynH0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
        "    # compute output shape of conv2D\n",
        "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
        "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
        "    return outshape\n",
        "\n",
        "\n",
        "# 2D CNN encoder train from scratch (no transfer learning)\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, img_x=90, img_y=120, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "\n",
        "        self.img_x = img_x\n",
        "        self.img_y = img_y\n",
        "        self.CNN_embed_dim = CNN_embed_dim\n",
        "\n",
        "        # CNN architechtures\n",
        "        self.ch1, self.ch2, self.ch3, self.ch4 = 32, 64, 128, 256\n",
        "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
        "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
        "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
        "\n",
        "        # conv2D output shapes\n",
        "        self.conv1_outshape = conv2D_output_size((self.img_x, self.img_y), self.pd1, self.k1, self.s1)  # Conv1 output shape\n",
        "        self.conv2_outshape = conv2D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
        "        self.conv3_outshape = conv2D_output_size(self.conv2_outshape, self.pd3, self.k3, self.s3)\n",
        "        self.conv4_outshape = conv2D_output_size(self.conv3_outshape, self.pd4, self.k4, self.s4)\n",
        "\n",
        "        # fully connected layer hidden nodes\n",
        "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1, padding=self.pd1),\n",
        "            nn.BatchNorm2d(self.ch1, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),                      \n",
        "            # nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2, padding=self.pd2),\n",
        "            nn.BatchNorm2d(self.ch2, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3, stride=self.s3, padding=self.pd3),\n",
        "            nn.BatchNorm2d(self.ch3, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=self.ch3, out_channels=self.ch4, kernel_size=self.k4, stride=self.s4, padding=self.pd4),\n",
        "            nn.BatchNorm2d(self.ch4, momentum=0.01),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.drop = nn.Dropout2d(self.drop_p)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(self.ch4 * self.conv4_outshape[0] * self.conv4_outshape[1], self.fc_hidden1)   # fully connected layer, output k classes\n",
        "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
        "        self.fc3 = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)   # output = CNN embedding latent variables\n",
        "\n",
        "    def forward(self, x_3d):\n",
        "        cnn_embed_seq = []\n",
        "        for t in range(x_3d.size(1)):\n",
        "            # CNNs\n",
        "            x = self.conv1(x_3d[:, t, :, :, :])\n",
        "            x = self.conv2(x)\n",
        "            x = self.conv3(x)\n",
        "            x = self.conv4(x)\n",
        "            x = x.view(x.size(0), -1)           # flatten the output of conv\n",
        "\n",
        "            # FC layers\n",
        "            x = F.relu(self.fc1(x))\n",
        "            # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "            x = self.fc3(x)\n",
        "            cnn_embed_seq.append(x)\n",
        "\n",
        "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
        "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
        "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
        "\n",
        "        return cnn_embed_seq\n",
        "\n",
        "\n",
        "# 2D CNN encoder using ResNet-152 pretrained\n",
        "class ResCNNEncoder(nn.Module):\n",
        "    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(ResCNNEncoder, self).__init__()\n",
        "\n",
        "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n",
        "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n",
        "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
        "        \n",
        "    def forward(self, x_3d):\n",
        "        cnn_embed_seq = []\n",
        "        for t in range(x_3d.size(1)):\n",
        "            # ResNet CNN\n",
        "            with torch.no_grad():\n",
        "                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n",
        "                x = x.view(x.size(0), -1)             # flatten output of conv\n",
        "\n",
        "            # FC layers\n",
        "            x = self.bn1(self.fc1(x))\n",
        "            x = F.relu(x)\n",
        "            x = self.bn2(self.fc2(x))\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "            x = self.fc3(x)\n",
        "\n",
        "            cnn_embed_seq.append(x)\n",
        "\n",
        "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
        "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
        "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
        "\n",
        "        return cnn_embed_seq\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.RNN_input_size = CNN_embed_dim\n",
        "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
        "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
        "        self.h_FC_dim = h_FC_dim\n",
        "        self.drop_p = drop_p\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.LSTM = nn.LSTM(\n",
        "            input_size=self.RNN_input_size,\n",
        "            hidden_size=self.h_RNN,        \n",
        "            num_layers=h_RNN_layers,       \n",
        "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
        "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, x_RNN):\n",
        "        \n",
        "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
        "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
        "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
        "\n",
        "        # FC layers\n",
        "        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnPQnQIatm8S",
        "colab_type": "text"
      },
      "source": [
        "# Generatinh Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyCfrGVicNL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_model(model_name='densenet',n_classes=5,resnet_shortcut='B',model_depth=121,sample_duration=16,sample_size=224,mode='score'):\n",
        "    assert mode in ['score', 'feature']\n",
        "    if mode == 'score':\n",
        "        last_fc = True\n",
        "    elif mode == 'feature':\n",
        "        last_fc = False\n",
        "\n",
        "    assert model_name in ['resnet', 'preresnet', 'wideresnet', 'resnext', 'densenet']\n",
        "\n",
        "    if model_name == 'resnet':\n",
        "        assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
        "\n",
        "        if model_depth == 10:\n",
        "            model = resnet10(num_classes=n_classes, shortcut_type=resnet_shortcut,\n",
        "                                    sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                    last_fc=last_fc)\n",
        "        elif model_depth == 18:\n",
        "            model = resnet18(num_classes=n_classes, shortcut_type=resnet_shortcut,\n",
        "                                    sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                    last_fc=last_fc)\n",
        "        elif model_depth == 34:\n",
        "            model = resnet34(num_classes=n_classes, shortcut_type=resnet_shortcut,\n",
        "                                    sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                    last_fc=last_fc)\n",
        "        elif model_depth == 50:\n",
        "            model = resnet50(num_classes=n_classes, shortcut_type=resnet_shortcut,\n",
        "                                    sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                    last_fc=last_fc)\n",
        "        elif model_depth == 101:\n",
        "            model = resnet101(num_classes=n_classes, shortcut_type=resnet_shortcut,\n",
        "                                    sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                    last_fc=last_fc)\n",
        "        elif model_depth == 152:\n",
        "            model = resnet152(num_classes=n_classes, shortcut_type=resnet_shortcut,\n",
        "                                    sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                    last_fc=last_fc)\n",
        "        elif model_depth == 200:\n",
        "            model = resnet200(num_classes=n_classes, shortcut_type=resnet_shortcut,\n",
        "                                    sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                    last_fc=last_fc)\n",
        "            \n",
        "    elif model_name == 'densenet':\n",
        "        assert model_depth in [121, 169, 201, 264]\n",
        "\n",
        "        if model_depth == 121:\n",
        "            model = densenet121(num_classes=n_classes,sample_size=sample_size, sample_duration=sample_duration,last_fc=last_fc)\n",
        "        elif model_depth == 169:\n",
        "            model = densenet169(num_classes=n_classes,\n",
        "                                         sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                         last_fc=last_fc)\n",
        "        elif model_depth == 201:\n",
        "            model = densenet201(num_classes=n_classes,\n",
        "                                         sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                         last_fc=last_fc)\n",
        "        elif model_depth == 264:\n",
        "            model = densenet264(num_classes=n_classes,\n",
        "                                         sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                         last_fc=last_fc)\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyU2MI0-vn67",
        "colab_type": "text"
      },
      "source": [
        "# Set train model & other parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mfl77KwpB0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######______________DenseNet 3D or ResNet 3D  ___________#############\n",
        "\n",
        "model_save_location_and_name = '/content/torch_model.pth'\n",
        " \n",
        "num_epochs = 5\n",
        "model = generate_model(model_name='densenet', n_classes=5, model_depth=121, sample_duration=16, sample_size=224, mode='score')\n",
        "#model.cuda()\n",
        "\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g5fooU8_M2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######______________Simple CNN 3D___________#############\n",
        "\n",
        "\n",
        "\n",
        "model_save_location_and_name = '/content/torch_model.pth'\n",
        " \n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "model = CNN3DModel()\n",
        "#model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = nn.CrossEntropyLoss()\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNwqQUeG_6ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######______________Encoder Decoder(ConvLSTM)___________#############\n",
        "\n",
        "\n",
        "\n",
        "model_save_location_and_name_encoder = '/content/encoder.pth'\n",
        "model_save_location_and_name_decoder = '/content/decoder.pth'\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
        "CNN_embed_dim = 512      # latent dim extracted by 2D CNN\n",
        "img_x, img_y = 224, 224  # resize video 2d frame size\n",
        "dropout_p = 0.0          # dropout probability\n",
        "\n",
        "# DecoderRNN architecture\n",
        "RNN_hidden_layers = 3\n",
        "RNN_hidden_nodes = 512\n",
        "RNN_FC_dim = 256\n",
        "\n",
        "\n",
        "cnn_encoder = EncoderCNN(img_x=img_x, img_y=img_y, fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2,\n",
        "                         drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim)\n",
        "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
        "                         h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=5)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = nn.CrossEntropyLoss()\n",
        "crnn_params = list(cnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(crnn_params, lr=learning_rate)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtVFR0Www4Xj",
        "colab_type": "text"
      },
      "source": [
        "# Traing model\n",
        "\n",
        "\n",
        "> DenseNet 3D or ResNet 3D  \n",
        "\n",
        "> Simple CNN 3D\n",
        "\n",
        "> Encoder Decoder(ConvLSTM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84dEmZftIqPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######_______________DenseNet 3D or ResNet 3D___________#############\n",
        "\n",
        "\n",
        "\n",
        "temp_accuracy = 0\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, samples in enumerate(train_loader):\n",
        "        \n",
        "        \n",
        "        imgs,imgs_road, lms = samples['image'],samples['image_road'], samples['label']\n",
        "        imgs = imgs.view(1,3,16,224,224)\n",
        "        imgs_road = imgs_road.view(1,3,16,224,224)\n",
        "        imgs = imgs.float()\n",
        "        imgs_road = imgs_road.float()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(imgs)\n",
        "\n",
        "\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, lms)\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        if count % 10 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for j, samples in enumerate(test_loader):\n",
        "                \n",
        "                imgs,imgs_road, lms = samples['image'],samples['image_road'], samples['label']\n",
        "                imgs = imgs.view(1,3,16,224,224)\n",
        "                imgs_road = imgs_road.view(1,3,16,224,224)\n",
        "                imgs = imgs.float()\n",
        "                imgs_road = imgs_road.float()\n",
        "\n",
        "                # Forward propagation\n",
        "                outputs = model(imgs)\n",
        "                \n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += len(lms)\n",
        "                correct += (predicted == lms).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / float(total)\n",
        "            if temp_accuracy<accuracy:\n",
        "              temp_accuracy = accuracy\n",
        "              torch.save(model.state_dict(), model_save_location_and_name )\n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        if count % 10 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npveNrqdESa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######______________Simple CNN 3D___________#############\n",
        "\n",
        "\n",
        "\n",
        "temp_accuracy = 0\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, samples in enumerate(train_loader):\n",
        "        \n",
        "        \n",
        "        imgs,imgs_road, lms = samples['image'],samples['image_road'], samples['label']\n",
        "        imgs = imgs.view(1,3,16,224,224)\n",
        "        imgs_road = imgs_road.view(1,3,16,224,224)\n",
        "        imgs = imgs.float()\n",
        "        imgs_road = imgs_road.float()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(imgs,imgs_road)\n",
        "\n",
        "\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, lms)\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        if count % 10 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for j, samples in enumerate(test_loader):\n",
        "                \n",
        "                imgs,imgs_road, lms = samples['image'],samples['image_road'], samples['label']\n",
        "                imgs = imgs.view(1,3,16,224,224)\n",
        "                imgs_road = imgs_road.view(1,3,16,224,224)\n",
        "                imgs = imgs.float()\n",
        "                imgs_road = imgs_road.float()\n",
        "\n",
        "                # Forward propagation\n",
        "                outputs = model(imgs,imgs_road)\n",
        "                \n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += len(lms)\n",
        "                correct += (predicted == lms).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / float(total)\n",
        "            if temp_accuracy<accuracy:\n",
        "              temp_accuracy = accuracy\n",
        "              torch.save(model.state_dict(), model_save_location_and_name )\n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        if count % 10 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3SVMKyHpe2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######______________Encoder Decoder(ConvLSTM)___________#############\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, samples in enumerate(train_loader):\n",
        "        \n",
        "        \n",
        "        imgs,imgs_road, lms = samples['image'],samples['image_road'], samples['label']\n",
        "        imgs = imgs.view(1,16,3,224,224)\n",
        "        imgs_road = imgs_road.view(1,16,3,224,224)\n",
        "        imgs = imgs.float()\n",
        "        imgs_road = imgs_road.float()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = rnn_decoder(cnn_encoder(imgs)) \n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, lms)\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        if count % 10 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for j, samples in enumerate(test_loader):\n",
        "                \n",
        "                imgs,imgs_road, lms = samples['image'],samples['image_road'], samples['label']\n",
        "                imgs = imgs.view(1,16,3,224,224)\n",
        "                imgs_road = imgs_road.view(1,16,3,224,224)\n",
        "                imgs = imgs.float()\n",
        "                imgs_road = imgs_road.float()\n",
        "\n",
        "\n",
        "                outputs = rnn_decoder(cnn_encoder(imgs)) \n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += len(lms)\n",
        "                correct += (predicted == lms).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / float(total)\n",
        "            \n",
        "            if temp_accuracy<accuracy:\n",
        "              temp_accuracy = accuracy\n",
        "              torch.save(cnn_encoder.state_dict(), model_save_location_and_name_encoder )\n",
        "              torch.save(rnn_decoder.state_dict(), model_save_location_and_name_decoder )\n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        if count % 10 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5fltwzuRXo",
        "colab_type": "text"
      },
      "source": [
        "# visualization loss & accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO0vyEHb3www",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list)\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"CNN: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}