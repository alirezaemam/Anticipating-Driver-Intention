{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_trainig.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPwNLT6gZjOe",
        "colab_type": "text"
      },
      "source": [
        "# Table of contents\n",
        "\n",
        "\n",
        "1.   import *libraries*\n",
        "2.   download data & extract file\n",
        "3.   Read File Name & Set label & Clean Data\n",
        "4.   Read Dataset\n",
        "5.   Image Augmenation\n",
        "6.   Randomize our data\n",
        "7.   Using KFOLD to split our data and labels\n",
        "8.   Create Train and Test Dataloaders\n",
        "\n",
        "\n",
        "\n",
        "9. Define Model\n",
        "\n",
        "> * CNN3DModel\n",
        "\n",
        "> * ResNet 3D\n",
        "\n",
        "> * DensNet 3D\n",
        "\n",
        "> * Encoder Decoder (ConvLSTM)\n",
        "\n",
        "\n",
        "10. Generating Model\n",
        "\n",
        "11. Set train model & other parameters\n",
        "\n",
        "12. Train model\n",
        "\n",
        "13. visualization loss & accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaN0iYCEWrb1",
        "colab_type": "text"
      },
      "source": [
        "# ***Import library***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGqPerb65mYQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5eff14eb-a011-46f6-9359-ac664c3ef8b4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pnd\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import math\n",
        "import os\n",
        "import zipfile\n",
        "import six\n",
        "import warnings\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import cv2\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as Data\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from scipy.io import loadmat\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3407872/45929032 bytes (7.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7020544/45929032 bytes (15.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10682368/45929032 bytes (23.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14385152/45929032 bytes (31.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18178048/45929032 bytes (39.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21864448/45929032 bytes (47.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25591808/45929032 bytes (55.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29114368/45929032 bytes (63.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32833536/45929032 bytes (71.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36626432/45929032 bytes (79.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40230912/45929032 bytes (87.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43778048/45929032 bytes (95.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnRUQLu0v38j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://uc0442da848516982bf140833a22.dl.dropboxusercontent.com/zip_download_get/AhsvTaMWi9zRKo9z4PzDqQvpvoBHETCw0kS3J1tdHnh-TVKjQxEXHOaFh1YXXvSbsoxU6m5QUbPDPli9nAfwxDdM3j5KTmPGMlU2WcGeJ7d0-w?_download_id=447858557738765268854499448370801617867357168088245854390127950184&_notify_domain=www.dropbox.com&dl=1\"\n",
        "target_path = '/content/brain.zip'\n",
        "import requests, zipfile, io\n",
        "response = requests.get(url, stream=True)\n",
        "handle = open(target_path, \"wb\")\n",
        "for chunk in response.iter_content(chunk_size=100):\n",
        "    if chunk:  # filter out keep-alive new chunks\n",
        "        handle.write(chunk)\n",
        "handle.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGsgJ6xhv60c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/brain.zip', 'r')\n",
        "zip_ref.extractall('/content/home/train')\n",
        "zip_ref.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8O9OdpXW3Ru",
        "colab_type": "text"
      },
      "source": [
        "# **Download Data & Extract File**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84Z-akE-Oe55",
        "colab_type": "text"
      },
      "source": [
        "# Read File Name & Set label & Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnOIm4-t3_WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "road = '/content/home/train/brain4cars_data/road_camera/' \n",
        "face = '/content/home/train/brain4cars_data/face_camera/'\n",
        "classes = os.listdir(face) # face and road have the same classes\n",
        "\n",
        "face_filename=[]\n",
        "road_filename=[]\n",
        "speed_filename=[]\n",
        "labels=[]\n",
        "\n",
        "for i in range(len(classes)):\n",
        "  path_face = face + classes[i]\n",
        "  path_road = road + classes[i]\n",
        "  face_check = os.listdir(path_face)\n",
        "  road_check = os.listdir(path_road)\n",
        "\n",
        "  for j in range(len(face_check)):\n",
        "    if face_check[j]+'.avi' in road_check:\n",
        "      video_face_path = path_face+'/'+face_check[j]+'/video_'+face_check[j]+'.avi'\n",
        "      video_road_path = path_road+'/'+face_check[j]+'.avi'\n",
        "      mat_speed_path = path_face+'/'+face_check[j]+'/params_'+face_check[j]+'.mat'\n",
        "      \n",
        "      try:\n",
        "        clip_face = VideoFileClip(video_face_path)\n",
        "        clip_road = VideoFileClip(video_road_path)\n",
        "        a = clip_face.duration\n",
        "        b = clip_road.duration\n",
        "      except:\n",
        "        a = 1 \n",
        "        b = 2\n",
        "      if a==b and a>5:\n",
        "        face_filename.append(video_face_path)\n",
        "        road_filename.append(video_road_path)\n",
        "        x = loadmat(mat_speed_path)\n",
        "        speed_filename.append(x[\"params\"][\"frame_data\"][0][0][0][-1][\"speed\"][0][0][0][0])\n",
        "        labels.append(i)\n",
        "        gc.collect()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcZ6vtPbXdiw",
        "colab_type": "text"
      },
      "source": [
        "# Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DWRG15dbtQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readframe (file_name, input_size, sample_rate, num_frames):\n",
        "  count =0\n",
        "  data=[]\n",
        "  cap = cv2.VideoCapture(file_name)\n",
        "  if not cap.isOpened():\n",
        "    print(\"Unable to connect to camera.\")\n",
        "  while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if ret == True and count%sample_rate==0 :\n",
        "      frame = cv2.resize(frame, (input_size, input_size), interpolation = cv2.INTER_AREA)\n",
        "      data.append(frame)\n",
        "      if ret == False or len(data)==num_frames:\n",
        "        break\n",
        "      count=count+ 1\n",
        "  return data\n",
        "\n",
        "class BrainforCarsDataset(Dataset):\n",
        "    def __init__(self, face_filename, road_filename,speed_filename, labels, input_size, sample_rate, num_frames, transform=None):\n",
        "        self.face_filename = face_filename\n",
        "        self.road_filename = road_filename\n",
        "        self.speed_filename = speed_filename\n",
        "        self.transform = transform\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.sample_rate = sample_rate\n",
        "        self.input_size = input_size\n",
        "        self.num_imgs = len(self.face_filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_imgs\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        data_face = readframe(self.face_filename[idx], self.input_size, self.sample_rate, self.num_frames)\n",
        "        data_face = np.array(data_face)\n",
        "\n",
        "        #data_road = readframe(self.road_filename[idx], self.input_size, self.sample_rate, self.num_frames)\n",
        "        #data_road = np.array(data_road)\n",
        "\n",
        "        lm = np.array(self.labels[idx])\n",
        "        speed = np.array(self.speed_filename[idx])\n",
        "\n",
        "\n",
        "        sample = {'image': data_face, \"speed\": speed, 'label': lm}\n",
        "        sample = {'image': data_road, \"speed\": speed, 'label': lm}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKD3W9MHZrj_",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations (all of these Augmentatios dont change the labels of the data)\n",
        "# https://gitee.com/alavaien/imgaug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt1rrlfV9N5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImgAugTransform(object):\n",
        "  def __init__(self):\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
        "    self.aug = iaa.Sequential(\n",
        "        [\n",
        "            # apply the following augmenters to most images\n",
        "            iaa.LinearContrast((2.0, 2.5)), \n",
        "            iaa.Invert(1, per_channel=True), \n",
        "            sometimes(iaa.Affine(\n",
        "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n",
        "                rotate=(-10, 10), # rotate by -45 to +45 degrees\n",
        "                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
        "                \n",
        "                mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "            )),\n",
        "            # execute 0 to 5 of the following (less important) augmenters per image\n",
        "            # don't execute all of them, as that would often be way too strong\n",
        "            iaa.SomeOf((0, 5),\n",
        "                [\n",
        "                    \n",
        "                    iaa.OneOf([\n",
        "                        iaa.GaussianBlur((0, 0.5)), # blur images with a sigma between 0 and 3.0\n",
        "                        iaa.AverageBlur(k=(1, 3)), # blur image using local means with kernel sizes between 2 and 7\n",
        "                        iaa.MedianBlur(k=(1, 3)), # blur image using local medians with kernel sizes between 2 and 7\n",
        "                    ]),\n",
        "                    iaa.Sharpen(alpha=(.9, 1.0), lightness=(0.5, 1.6)), # sharpen images\n",
        "                    \n",
        "                    # search either for all edges or for directed edges,\n",
        "                    # blend the result with the original image using a blobby mask\n",
        "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "                        iaa.EdgeDetect(alpha=(0.0, 0.2)),\n",
        "                        \n",
        "                    ])),\n",
        "                    \n",
        "                    iaa.OneOf([\n",
        "                        iaa.Dropout((0.01, 0.03), per_channel=0.5), # randomly remove up to 10% of the pixels                        \n",
        "                    ]),\n",
        "                    iaa.Invert(0.01, per_channel=True), # invert color channels\n",
        "                    iaa.Add((-2, 2), per_channel=0.5), # change brightness of images (by -2 to 2 of original value)\n",
        "                    iaa.AddToHueAndSaturation((-1, 1)), # change hue and saturation - add blue light\n",
        "                    # either change the brightness of the whole image (sometimes\n",
        "                    # per channel) or change the brightness of subareas\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "                        iaa.FrequencyNoiseAlpha( exponent=(-1, 0),first=iaa.Multiply((0.9, 1.1), per_channel=True),  # add dark light\n",
        "                        second=iaa.ContrastNormalization((0.5, 1.5))\n",
        "                        )\n",
        "                    ]),\n",
        "                    sometimes(iaa.ElasticTransformation(alpha=(0.3, 0.5), sigma=0.2)), # move pixels locally around (with random strengths)\n",
        "                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.02))), # sometimes move parts of the image around\n",
        "                    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.05))) # change perspective\n",
        "                ],\n",
        "                random_order=True\n",
        "            )\n",
        "        ],\n",
        "        random_order=True\n",
        "    )\n",
        "      \n",
        "  def __call__(self, sample):\n",
        "    img = sample['image']\n",
        "    img = img.astype(np.uint8)  #imgaug works with np.unit8\n",
        "    img = torch.from_numpy(self.aug.augment_images(img).copy())\n",
        "\n",
        "\n",
        "    sample_1 = {'image': img, \"speed\": sample[\"speed\"], 'label':sample['label']}\n",
        "    return sample_1 \n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hmo-ORRf3p",
        "colab_type": "text"
      },
      "source": [
        "# Randomize our data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8oExICiOzFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(1254)\n",
        "\n",
        "combined = list(zip(face_filename, road_filename, speed_filename, labels))\n",
        "random.shuffle(combined)\n",
        "\n",
        "face_filename, road_filename, speed_filename, labels = zip(*combined)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VuUu2LqRuxE",
        "colab_type": "text"
      },
      "source": [
        "# Using StratifiedKFold to split our data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVO_yXHOS-Eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold = 0\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n",
        "face_filename = np.array(face_filename)\n",
        "road_filename = np.array(road_filename)\n",
        "speed_filename = np.array(speed_filename)\n",
        "labels = np.array(labels)\n",
        "\n",
        "for i ,(train_indices, test_indices) in enumerate(skf.split(face_filename, labels)):\n",
        "  if i ==n_fold:\n",
        "    face_filename_train = face_filename[train_indices]\n",
        "    face_filename_test = face_filename[test_indices]\n",
        "    road_filename_train = road_filename[train_indices]\n",
        "    road_filename_test = road_filename[test_indices]\n",
        "    speed_filename_train = speed_filename[train_indices]\n",
        "    speed_filename_test = speed_filename[test_indices]\n",
        "    labels_train = labels[train_indices]\n",
        "    labels_test = labels[test_indices]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEHDy_O-SqWw",
        "colab_type": "text"
      },
      "source": [
        "# Train and Test Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewr5atlRVPgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_loader(input_size, sample_rate, num_frames):\n",
        "    ImgAug = ImgAugTransform()\n",
        "    composed = transforms.Compose([ImgAug])\n",
        "    train_data = BrainforCarsDataset(face_filename_train,road_filename_train, speed_filename_train, labels_train, input_size, sample_rate, num_frames, transform=composed)\n",
        "    train_loader = Data.DataLoader(train_data, batch_size=24, shuffle=False, num_workers=0)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def get_test_loader(input_size, sample_rate, num_frames):\n",
        "    test_data = BrainforCarsDataset(face_filename_test, road_filename_test, speed_filename_test, labels_test, input_size, sample_rate, num_frames, transform=None)\n",
        "    test_loader = Data.DataLoader(test_data, batch_size=24, shuffle=False, num_workers=0)\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "train_loader = get_train_loader(224, 9, 16)\n",
        "test_loader = get_test_loader(224, 9, 16)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXDT27hYbUXJ",
        "colab_type": "text"
      },
      "source": [
        "# Define Model\n",
        "\n",
        "> CNN3DModel\n",
        "\n",
        "> ResNet 3D\n",
        "\n",
        "> DensNet 3D\n",
        "\n",
        "> Encoder Decoder (ConvLSTM)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzn4RGsJbRyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet264']\n",
        "\n",
        "\n",
        "def get_fine_tuning_parameters(model, ft_begin_index):\n",
        "    if ft_begin_index == 0:\n",
        "        return model.parameters()\n",
        "\n",
        "    ft_module_names = []\n",
        "    for i in range(ft_begin_index, 5):\n",
        "        ft_module_names.append('denseblock{}'.format(ft_begin_index))\n",
        "        ft_module_names.append('transition{}'.format(ft_begin_index))\n",
        "    ft_module_names.append('norm5')\n",
        "    ft_module_names.append('classifier')\n",
        "\n",
        "    parameters = []\n",
        "    for k, v in model.named_parameters():\n",
        "        for ft_module in ft_module_names:\n",
        "            if ft_module in k:\n",
        "                parameters.append({'params': v})\n",
        "                break\n",
        "        else:\n",
        "            parameters.append({'params': v, 'lr': 0.0})\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Sequential):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm3d(num_input_features))\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv1', nn.Conv3d(num_input_features, bn_size * growth_rate,\n",
        "                                            kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv2', nn.Conv3d(bn_size * growth_rate, growth_rate,\n",
        "                                            kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_features = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return torch.cat([x, new_features], 1)\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv3d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    \"\"\"Densenet-BC model class\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_size, sample_duration, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, last_fc=True):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        self.last_fc = last_fc\n",
        "\n",
        "        self.sample_size = sample_size\n",
        "        self.sample_duration = sample_duration\n",
        "\n",
        "        # First convolution\n",
        "        # self.features = nn.Sequential(OrderedDict([\n",
        "        #     ('conv0', nn.Conv3d(3, num_init_features, kernel_size=7,\n",
        "        #                         stride=(1, 2, 2), padding=(3, 3, 3), bias=False)),\n",
        "        #     ('norm0', nn.BatchNorm3d(num_init_features)),\n",
        "        #     ('relu0', nn.ReLU(inplace=True)),\n",
        "        #     ('pool0', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)),\n",
        "        # ]))\n",
        "\n",
        "\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(4, num_init_features, kernel_size=7,\n",
        "                                stride=(1, 2, 2), padding=(3, 3, 3), bias=False),\n",
        "            nn.BatchNorm3d(num_init_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        last_duration = math.ceil(self.sample_duration / self.sample_duration)\n",
        "        last_size = math.floor(self.sample_size / 32)\n",
        "        out = F.avg_pool3d(out, kernel_size=(last_duration, last_size, last_size)).view(features.size(0), -1)\n",
        "        if self.last_fc:\n",
        "            out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "def densenet121(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                     **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet169(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n",
        "                     **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet201(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n",
        "                     **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet264(**kwargs):\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 64, 48),\n",
        "                     **kwargs)\n",
        "    return model\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH1cX9LynH0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
        "    # compute output shape of conv2D\n",
        "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
        "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
        "    return outshape\n",
        "\n",
        "class ResCNNEncoder(nn.Module):\n",
        "    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(ResCNNEncoder, self).__init__()\n",
        "\n",
        "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1] \n",
        "        modules[0] = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)    \n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(fc_hidden1)\n",
        "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(fc_hidden2)\n",
        "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
        "        \n",
        "        \n",
        "    def forward(self, x_3d):\n",
        "        cnn_embed_seq = []\n",
        "        for t in range(x_3d.size(1)):\n",
        "            # ResNet CNN\n",
        "            with torch.no_grad():\n",
        "                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n",
        "                x = x.view(x.size(0), -1)             # flatten output of conv\n",
        "\n",
        "            # FC layers\n",
        "            x = self.fc1(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.fc2(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "            x = self.fc3(x)\n",
        "\n",
        "            cnn_embed_seq.append(x)\n",
        "\n",
        "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
        "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
        "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
        "\n",
        "        return cnn_embed_seq\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.RNN_input_size = CNN_embed_dim\n",
        "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
        "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
        "        self.h_FC_dim = h_FC_dim\n",
        "        self.drop_p = drop_p\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.LSTM = nn.LSTM(\n",
        "            input_size=self.RNN_input_size,\n",
        "            hidden_size=self.h_RNN,        \n",
        "            num_layers=h_RNN_layers,       \n",
        "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
        "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
        "\n",
        "    def forward(self, x_RNN):\n",
        "        \n",
        "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
        "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
        "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
        "\n",
        "        # FC layers\n",
        "        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnPQnQIatm8S",
        "colab_type": "text"
      },
      "source": [
        "# Generating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyCfrGVicNL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_model(model_name='densenet',n_classes=5,model_depth=161,sample_duration=16,sample_size=224,mode='score'):\n",
        "    assert mode in ['score', 'feature']\n",
        "    if mode == 'score':\n",
        "        last_fc = True\n",
        "    elif mode == 'feature':\n",
        "        last_fc = False\n",
        "  \n",
        "    if model_name == 'densenet':\n",
        "        assert model_depth in [121, 169, 201, 264]\n",
        "\n",
        "        if model_depth == 121:\n",
        "            model = densenet121(num_classes=n_classes,sample_size=sample_size, sample_duration=sample_duration,last_fc=last_fc)\n",
        "        elif model_depth == 169:\n",
        "            model = densenet169(num_classes=n_classes,\n",
        "                                         sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                         last_fc=last_fc)\n",
        "        elif model_depth == 201:\n",
        "            model = densenet201(num_classes=n_classes,\n",
        "                                         sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                         last_fc=last_fc)\n",
        "        elif model_depth == 264:\n",
        "            model = densenet264(num_classes=n_classes,\n",
        "                                         sample_size=sample_size, sample_duration=sample_duration,\n",
        "                                         last_fc=last_fc)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyU2MI0-vn67",
        "colab_type": "text"
      },
      "source": [
        "# Set train model & other parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mfl77KwpB0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######______________DenseNet 3D  ___________#############\n",
        "\n",
        "model_save_location_and_name = '/content/torch_model.pth'\n",
        " \n",
        "num_epochs = 150\n",
        "model = generate_model(model_name='densenet', n_classes=5, model_depth=121, sample_duration=16, sample_size=224, mode='score')\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.0005\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNwqQUeG_6ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######______________Encoder Decoder(ConvLSTM)___________#############\n",
        "\n",
        "\n",
        "model_save_location_and_name_encoder = '/content/encoder.pth'\n",
        "model_save_location_and_name_decoder = '/content/decoder.pth'\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "num_epochs = 150\n",
        "\n",
        "\n",
        "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
        "CNN_embed_dim = 512      # latent dim extracted by 2D CNN\n",
        "img_x, img_y = 224, 224  # resize video 2d frame size\n",
        "dropout_p = 0.0          # dropout probability\n",
        "\n",
        "# DecoderRNN architecture\n",
        "RNN_hidden_layers = 3\n",
        "RNN_hidden_nodes = 512\n",
        "RNN_FC_dim = 256\n",
        "\n",
        "\n",
        "cnn_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, CNN_embed_dim=CNN_embed_dim)\n",
        "rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, h_FC_dim=RNN_FC_dim, num_classes=5)\n",
        "\n",
        "cnn_encoder.cuda()\n",
        "rnn_decoder.cuda()\n",
        "\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = nn.CrossEntropyLoss()\n",
        "crnn_params = list(cnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.0005\n",
        "optimizer = torch.optim.SGD(crnn_params, lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtVFR0Www4Xj",
        "colab_type": "text"
      },
      "source": [
        "# Traing model\n",
        "\n",
        "\n",
        "> DenseNet 3D or ResNet 3D  \n",
        "\n",
        "> Simple CNN 3D\n",
        "\n",
        "> Encoder Decoder(ConvLSTM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84dEmZftIqPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######_______________DenseNet 3D----- FACE___________#############\n",
        "\n",
        "temp_accuracy = 0\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, samples in enumerate(train_loader):\n",
        "        \n",
        "        imgs_face, speed, lms = samples['image'], samples[\"speed\"], samples['label']\n",
        "        imgs_face = imgs_face.view(24,3,16,224,224)\n",
        "        a = torch.empty((24, 1, 16, 224, 224))\n",
        "        a = torch.zeros_like(a) + speed\n",
        "        imgs_face = torch.cat((imgs_face, a), dim =1)\n",
        "        imgs_face = imgs_face.float()\n",
        "        imgs_face = imgs_face.cuda()\n",
        "        lms=lms.long()\n",
        "        lms = lms.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(imgs_face)\n",
        "\n",
        "\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, lms)\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        if count % 10 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for j, samples in enumerate(test_loader):\n",
        "\n",
        "                imgs_face, speed, lms = samples['image'], samples[\"speed\"], samples['label']\n",
        "                imgs_face = imgs_face.view(24,3,16,224,224)\n",
        "                a = torch.empty((24, 1, 16, 224, 224))\n",
        "                a = torch.zeros_like(a) + speed\n",
        "                imgs_face = torch.cat((imgs_face, a), dim =1)\n",
        "                imgs_face = imgs_face.float()\n",
        "                imgs_face = imgs_face.cuda()\n",
        "                lms = lms.cuda()\n",
        "                # Forward propagation\n",
        "                outputs = model(imgs_face)\n",
        "                \n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += len(lms)\n",
        "                correct += (predicted == lms).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / float(total)\n",
        "            if temp_accuracy<accuracy:\n",
        "              temp_accuracy = accuracy\n",
        "              torch.save(model.state_dict(), model_save_location_and_name )\n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        if count % 10 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3SVMKyHpe2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "378c3633-969d-4aaf-fc2c-1f6265930c4e"
      },
      "source": [
        "######______________Encoder Decoder(ConvLSTM) -------road_data_________#############\n",
        "\n",
        "temp_accuracy=0\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, samples in enumerate(train_loader):\n",
        "        \n",
        "        imgs_road, speed, lms = samples['image'], samples[\"speed\"], samples['label']\n",
        "        imgs_road = imgs_road.view(24,16,3,224,224)\n",
        "        a = torch.empty((24, 16, 1, 224, 224))\n",
        "        a = torch.zeros_like(a) + speed  \n",
        "        imgs_road = torch.cat((imgs_road, a), dim =2)\n",
        "\n",
        "\n",
        "\n",
        "        imgs_road = imgs_road.float()\n",
        "        imgs_road = imgs_road.cuda()\n",
        "\n",
        "        lms = lms.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = rnn_decoder(cnn_encoder(imgs_road)) \n",
        "        # outputs = rnn_decoder(outputs)\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, lms)\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        if count % 10 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for j, samples in enumerate(test_loader):\n",
        "                imgs_road, speed, lms = samples['image'], samples[\"speed\"], samples['label']\n",
        "                imgs_road = imgs_road.view(24,16,3,224,224)\n",
        "                a = torch.empty((24, 16, 1, 224, 224))\n",
        "                a = torch.zeros_like(a) + speed  \n",
        "                imgs_road = torch.cat((imgs_road, a), dim =2)\n",
        "                imgs_road = imgs_road.float()\n",
        "\n",
        "                imgs_road = imgs_road.cuda()\n",
        "                lms = lms.cuda()\n",
        "\n",
        "\n",
        "                outputs = rnn_decoder(cnn_encoder(imgs_road)) \n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += len(lms)\n",
        "                correct += (predicted == lms).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / float(total)\n",
        "            \n",
        "            if temp_accuracy<accuracy:\n",
        "              temp_accuracy = accuracy\n",
        "              torch.save(cnn_encoder.state_dict(), model_save_location_and_name_encoder )\n",
        "              torch.save(rnn_decoder.state_dict(), model_save_location_and_name_decoder )\n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        if count % 10 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-69f909e050bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_road\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# outputs = rnn_decoder(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Calculate softmax and ross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-316f120d27da>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_3d)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# ResNet CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ResNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# flatten output of conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2015\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m     )\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5fltwzuRXo",
        "colab_type": "text"
      },
      "source": [
        "# visualization loss & accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO0vyEHb3www",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list)\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"CNN: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJkI5H3mB4oo",
        "colab_type": "text"
      },
      "source": [
        "# Feature Fusion\n",
        "\n",
        "\n",
        "\n",
        "> load model\n",
        "\n",
        "\n",
        "> class feature fusion\n",
        "\n",
        "\n",
        "> train \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPcrWKXS240B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BrainforCarsDataset(Dataset):\n",
        "    def __init__(self, face_filename, road_filename,speed_filename, labels, input_size, sample_rate, num_frames, transform=None):\n",
        "        self.face_filename = face_filename\n",
        "        self.road_filename = road_filename\n",
        "        self.speed_filename = speed_filename\n",
        "        self.transform = transform\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.sample_rate = sample_rate\n",
        "        self.input_size = input_size\n",
        "        self.num_imgs = len(self.face_filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_imgs\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        count =0\n",
        "        data_face=[]\n",
        "        cap = cv2.VideoCapture(self.face_filename[idx])\n",
        "        if not cap.isOpened():\n",
        "            print(\"Unable to connect to camera.\")\n",
        "        while cap.isOpened():\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "            \n",
        "            if ret == True and count%self.sample_rate==0 :\n",
        "              frame = cv2.resize(frame, (self.input_size, self.input_size), interpolation = cv2.INTER_AREA)\n",
        "              data_face.append(frame)\n",
        "\n",
        "            if ret == False or len(data_face)==self.num_frames:\n",
        "              break\n",
        "            count=count+ 1\n",
        "\n",
        "        \n",
        "        count =0\n",
        "        data_road=[]\n",
        "        cap = cv2.VideoCapture(self.road_filename[idx])\n",
        "        if not cap.isOpened():\n",
        "            print(\"Unable to connect to camera.\")\n",
        "        while cap.isOpened():\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "            if ret == True and count%self.sample_rate==0 :\n",
        "              frame = cv2.resize(frame, (self.input_size, self.input_size), interpolation = cv2.INTER_AREA)\n",
        "              data_road.append(frame)\n",
        "\n",
        "            if ret == False or len(data_road)== self.num_frames :\n",
        "              break\n",
        "            count=count+ 1\n",
        "    \n",
        "\n",
        "        lm = np.array(self.labels[idx])\n",
        "        speed = np.array(self.speed_filename[idx])\n",
        "        data_face = np.array(data_face)\n",
        "        data_road = np.array(data_road)\n",
        "        sample = {'image_face': data_face,'image_road': data_road, \"speed\": speed, 'label': lm}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_x9-S8J3Bf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImgAugTransform(object):\n",
        "  def __init__(self):\n",
        "    sometimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
        "    self.aug = iaa.Sequential(\n",
        "        [\n",
        "            # apply the following augmenters to most images\n",
        "            iaa.LinearContrast((2.0, 2.5)), \n",
        "            iaa.Invert(1, per_channel=True), \n",
        "            sometimes(iaa.Affine(\n",
        "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n",
        "                rotate=(-10, 10), # rotate by -45 to +45 degrees\n",
        "                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
        "                \n",
        "                mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "            )),\n",
        "            # execute 0 to 5 of the following (less important) augmenters per image\n",
        "            # don't execute all of them, as that would often be way too strong\n",
        "            iaa.SomeOf((0, 5),\n",
        "                [\n",
        "                    \n",
        "                    iaa.OneOf([\n",
        "                        iaa.GaussianBlur((0, 0.5)), # blur images with a sigma between 0 and 3.0\n",
        "                        iaa.AverageBlur(k=(1, 3)), # blur image using local means with kernel sizes between 2 and 7\n",
        "                        iaa.MedianBlur(k=(1, 3)), # blur image using local medians with kernel sizes between 2 and 7\n",
        "                    ]),\n",
        "                    iaa.Sharpen(alpha=(.9, 1.0), lightness=(0.5, 1.6)), # sharpen images\n",
        "                    \n",
        "                    # search either for all edges or for directed edges,\n",
        "                    # blend the result with the original image using a blobby mask\n",
        "                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "                        iaa.EdgeDetect(alpha=(0.0, 0.2)),\n",
        "                        \n",
        "                    ])),\n",
        "                    \n",
        "                    iaa.OneOf([\n",
        "                        iaa.Dropout((0.01, 0.03), per_channel=0.5), # randomly remove up to 10% of the pixels                        \n",
        "                    ]),\n",
        "                    iaa.Invert(0.01, per_channel=True), # invert color channels\n",
        "                    iaa.Add((-2, 2), per_channel=0.5), # change brightness of images (by -2 to 2 of original value)\n",
        "                    iaa.AddToHueAndSaturation((-1, 1)), # change hue and saturation - add blue light\n",
        "                    # either change the brightness of the whole image (sometimes\n",
        "                    # per channel) or change the brightness of subareas\n",
        "                    iaa.OneOf([\n",
        "                        iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "                        iaa.FrequencyNoiseAlpha( exponent=(-1, 0),first=iaa.Multiply((0.9, 1.1), per_channel=True),  # add dark light\n",
        "                        second=iaa.ContrastNormalization((0.5, 1.5))\n",
        "                        )\n",
        "                    ]),\n",
        "                    sometimes(iaa.ElasticTransformation(alpha=(0.3, 0.5), sigma=0.2)), # move pixels locally around (with random strengths)\n",
        "                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.02))), # sometimes move parts of the image around\n",
        "                    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.05))) # change perspective\n",
        "                ],\n",
        "                random_order=True\n",
        "            )\n",
        "        ],\n",
        "        random_order=True\n",
        "    )\n",
        "      \n",
        "  def __call__(self, sample):\n",
        "    img_face = sample['image_face']\n",
        "    img_face = img_face.astype(np.uint8)  #imgaug works with np.unit8\n",
        "    img_face = torch.from_numpy(self.aug.augment_images(img_face).copy())\n",
        "\n",
        "    img_road = sample['image_road']\n",
        "    img_road = img_road.astype(np.uint8)\n",
        "    img_road = torch.from_numpy(self.aug.augment_images(img_road).copy())\n",
        "\n",
        "    sample_1 = {'image_face': img_face, 'image_road': img_road, \"speed\": sample[\"speed\"], 'label':sample['label']}\n",
        "    return sample_1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rshHtqHH3Jo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_loader(input_size, sample_rate, num_frames):\n",
        "    ImgAug = ImgAugTransform()\n",
        "    composed = transforms.Compose([ImgAug])\n",
        "    train_data = BrainforCarsDataset(face_filename_train,road_filename_train, speed_filename_train, labels_train, input_size, sample_rate, num_frames, transform=composed)\n",
        "    train_loader = Data.DataLoader(train_data, batch_size=1, shuffle=False, num_workers=0)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def get_test_loader(input_size, sample_rate, num_frames):\n",
        "    test_data = BrainforCarsDataset(face_filename_test, road_filename_test, speed_filename_test, labels_test, input_size, sample_rate, num_frames, transform=None)\n",
        "    test_loader = Data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=0)\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "train_loader = get_train_loader(224, 5, 30)\n",
        "test_loader = get_test_loader(224, 5, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV8WodhvbxRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_location_and_name = '/content/torch_model.pth'\n",
        "model_face = generate_model(model_name='densenet', n_classes=5, model_depth=121, sample_duration=30, sample_size=224, mode='score')\n",
        "model_face.load_state_dict(torch.load(model_save_location_and_name))\n",
        "\n",
        "\n",
        "\n",
        "model_save_location_and_name_encoder = '/content/encoder.pth'\n",
        "model_save_location_and_name_decoder = '/content/decoder.pth'\n",
        "\n",
        "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
        "CNN_embed_dim = 512      # latent dim extracted by 2D CNN\n",
        "dropout_p = 0.0          # dropout probability\n",
        "\n",
        "# DecoderRNN architecture\n",
        "RNN_hidden_layers = 3\n",
        "RNN_hidden_nodes = 512\n",
        "RNN_FC_dim = 256\n",
        "\n",
        "model_encoder = ResCNNEncoder(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, CNN_embed_dim=CNN_embed_dim)\n",
        "model_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, h_FC_dim=RNN_FC_dim, num_classes=5)\n",
        "\n",
        "\n",
        "model_encoder.load_state_dict(torch.load(model_save_location_and_name_encoder))\n",
        "model_decoder.load_state_dict(torch.load(model_save_location_and_name_decoder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OUb8U2gB35k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Fusion(nn.Module):\n",
        "  def __init__(self, model_face, model_encoder, model_decoder):\n",
        "    super(Cnn, self).__init__()\n",
        "    self.model_face = model_face\n",
        "    self.model_face = nn.Sequential(*list(self.model_face.children())[:-1])\n",
        "\n",
        "    self.model_encoder = model_encoder\n",
        "    self.model_decoder = model_decoder\n",
        "    self.model_decoder = nn.Sequential(*list(self.model_decoder.children())[:-2])\n",
        "\n",
        "    self.fc1 = nn.Linear(50688, 2048)\n",
        "    self.drop_1 = nn.Dropout(0.4)\n",
        "    self.fc2 = nn.Linear(2048, 1024)\n",
        "    self.drop_2 = nn.Dropout(0.2)\n",
        "    self.fc3 = nn.Linear(1024, 5)\n",
        "\n",
        "  def forward (self, face, road):\n",
        "    x = self.model_face(face)\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    road = self.encoder(road)\n",
        "    RNN_out, (h_n, h_c) = self.model_decoder(road)\n",
        "    y = RNN_out[:, -1, :]\n",
        "    y = y.view(y.size(0), -1)\n",
        "\n",
        "    out = torch.cat((x,y),dim=1)\n",
        "    out = self.fc1(out)\n",
        "    out =  self.drop_1(out)\n",
        "    out = self.fc2(out)\n",
        "    out =  self.drop_2(out)\n",
        "    out = self.fc3(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuQpKSaykmXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fusionmodel = Fusion(model_face, model_encoder, model_decoder)\n",
        "num_epochs = 50\n",
        "Fusionmodel.cuda()\n",
        "\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.SGD(Fusionmodel.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB5xzsJnDmXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "e6ec870a-0a45-42f8-f828-d9dbd75b1f70"
      },
      "source": [
        "######______________Feature fusion _________#############\n",
        "\n",
        "temp_accuracy=0\n",
        "count = 0\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, samples in enumerate(train_loader):\n",
        "        \n",
        "        imgs_face, imgs_road, speed, lms = samples['image_face'],samples['image_road'], samples[\"speed\"], samples['label']\n",
        "\n",
        "        imgs_face = imgs_face.view(24, 3, 16,224,224)\n",
        "        a = torch.empty((24, 1, 16, 224, 224))\n",
        "        a = torch.zeros_like(a) + speed\n",
        "        imgs_face = torch.cat((imgs_face, a), dim =1)\n",
        "        imgs_face = imgs_face.float()\n",
        "        imgs_face = imgs_face.cuda()\n",
        "\n",
        "        imgs_road = imgs_road.view(24, 16, 3,224,224)\n",
        "        a = torch.empty((24, 16, 1, 224, 224))\n",
        "        a = torch.zeros_like(a) + speed  \n",
        "        imgs_road = torch.cat((imgs_road, a), dim =2)\n",
        "        imgs_road = imgs_road.float()\n",
        "        imgs_road = imgs_road.cuda()\n",
        "        lms = lms.long()\n",
        "        lms = lms.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = Fusionmodel(imgs_face,imgs_road)\n",
        "\n",
        "        # outputs = rnn_decoder(outputs)\n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, lms)\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        if count % 10 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for j, samples in enumerate(test_loader):\n",
        "                imgs_face, imgs_road, speed, lms = samples['image_face'],samples['image_road'], samples[\"speed\"], samples['label']\n",
        "\n",
        "                imgs_face = imgs_face.view(24,3,16,224,224)\n",
        "                a = torch.empty((24, 1, 16, 224, 224))\n",
        "                a = torch.zeros_like(a) + speed\n",
        "                imgs_face = torch.cat((imgs_face, a), dim =1)\n",
        "                imgs_face = imgs_face.float()\n",
        "                imgs_face = imgs_face.cuda()\n",
        "\n",
        "                imgs_road = imgs_road.view(24,16,3,224,224)\n",
        "                a = torch.empty((24, 16, 1 ,224, 224))\n",
        "                a = torch.zeros_like(a) + speed  \n",
        "                imgs_road = torch.cat((imgs_road, a), dim =2)\n",
        "                imgs_road = imgs_road.float()\n",
        "                imgs_road = imgs_road.cuda()\n",
        "                lms = lms.long()\n",
        "                lms = lms.cuda()\n",
        "\n",
        "                outputs = Fusionmodel(imgs_face, imgs_road)\n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += len(lms)\n",
        "                correct += (predicted == lms).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / float(total)\n",
        "            \n",
        "            if temp_accuracy<accuracy:\n",
        "              temp_accuracy = accuracy\n",
        "              torch.save(cnn_encoder.state_dict(), model_save_location_and_name_encoder )\n",
        "              torch.save(rnn_decoder.state_dict(), model_save_location_and_name_decoder )\n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "        if count % 10 == 0:\n",
        "            # Print Loss\n",
        "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50688])\n",
            "torch.Size([1, 50688])\n",
            "torch.Size([1, 50688])\n",
            "torch.Size([1, 50688])\n",
            "torch.Size([1, 50688])\n",
            "torch.Size([1, 50688])\n",
            "torch.Size([1, 50688])\n",
            "torch.Size([1, 50688])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-ff1f089dc5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimgs_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_road\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_face'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_road'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"speed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-8d9dd4e1d872>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD0hQWxTcZSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list)\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"CNN: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}